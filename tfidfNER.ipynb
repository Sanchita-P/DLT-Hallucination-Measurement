{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"lamini/lamini_docs\"\n",
    "datasets = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for qa_pair in datasets['train']:\n",
    "    _sentence = (qa_pair['question'] + ' ' + qa_pair['answer']).replace(\"\\\\n\", \" \")\n",
    "    data.append(_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             term        rank\n",
      "1848       lamini  103.728120\n",
      "3253         text   54.182434\n",
      "834          data   47.727913\n",
      "2100        model   41.152211\n",
      "142            ai   39.693581\n",
      "...           ...         ...\n",
      "3523     welcomes    0.066917\n",
      "273     attending    0.066917\n",
      "816     curiosity    0.066917\n",
      "2385        plays    0.066917\n",
      "888   departments    0.066917\n",
      "\n",
      "[3574 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "def top_tfidf_words(sentences, top_n=100):\n",
    "    # Initialize a TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    # Fit and transform the sentences\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # Get feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Sum TF-IDF for each term across all documents\n",
    "    sums = tfidf_matrix.sum(axis=0)\n",
    "\n",
    "    # Create a dataframe with words and their corresponding sums\n",
    "    data = []\n",
    "    for col, term in enumerate(feature_names):\n",
    "        data.append((term, sums[0, col]))\n",
    "\n",
    "    ranking = pd.DataFrame(data, columns=['term', 'rank'])\n",
    "    ranking = ranking.sort_values('rank', ascending=False)\n",
    "\n",
    "    return ranking\n",
    "\n",
    "# Get top 100 TF-IDF words\n",
    "top_words = top_tfidf_words(data)\n",
    "print(top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_words = []\n",
    "with open(\"/nethome/ss651/Robust-LLM/intent/glove.6B.100d.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        word = line.split()[0]\n",
    "        glove_words.append(word)\n",
    "\n",
    "important_words = top_words[\"term\"].tolist()\n",
    "\n",
    "filtered_words = []\n",
    "\n",
    "for word in important_words:\n",
    "    if word not in glove_words:\n",
    "        filtered_words.append(word)\n",
    "\n",
    "filtered_words =  sorted(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"train_ners.csv\")\n",
    "ner_list = list(df[\"NERS\"].dropna())\n",
    "\n",
    "ners = []\n",
    "for ner in ner_list:\n",
    "    ners.extend(ner.lower().replace(\"`\",\"\").split(\", \"))\n",
    "\n",
    "filtered_ners = []\n",
    "for ner in ners:\n",
    "    if ner not in glove_words:\n",
    "        filtered_ners.append(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('important_words.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for _word in filtered_words:\n",
    "        writer.writerow([_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "document_content = ' '.join(data)\n",
    "\n",
    "# Regex pattern for function calls\n",
    "pattern = r'\\b[a-zA-Z0-9]+\\w*_+\\w+?(?<!\\.py)\\b|\\b[a-zA-Z0-9]+\\w+\\(\\)(?<!\\.py\\b)'\n",
    "\n",
    "entities = re.findall(pattern, document_content)\n",
    "entities = sorted(list(set(entities)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it possible to customize the level of specificity in the generated output? Yes, it is possible to customize the level of specificity in the generated output. This can be achieved by adjusting the input parameters and output type in the LLM Engine function, as demonstrated in the \"TestOutputStr\" class in the \"test_output_str.py\" file. By defining specific input parameters and output types, the generated output can be tailored to meet the desired level of specificity.\n"
     ]
    }
   ],
   "source": [
    "for x in data:\n",
    "    if  'test_output_str' in x:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_entities = [\n",
    " 'add_data',\n",
    " 'add_improve_statements',\n",
    " 'add_metric',\n",
    " 'add_model',\n",
    " 'bad_examples',\n",
    " 'cancel_job',\n",
    " 'check_job_status',\n",
    " 'circular_operation',\n",
    " 'compare_equal_metric',\n",
    " 'configure_llama',\n",
    " 'edit_config',\n",
    " 'error_handling',\n",
    " 'filter_fn',\n",
    " 'full_balance_dataset',\n",
    " 'gen_queue_batch',\n",
    " 'gen_submit_training_job',\n",
    " 'get_job_results',\n",
    " 'get_response',\n",
    " 'good_examples',\n",
    " 'improve()',\n",
    " 'is_peft_model',\n",
    " 'length_penalty',\n",
    " 'llm()',\n",
    " 'make_discriminator',\n",
    " 'make_questions',\n",
    " 'max_retries',\n",
    " 'max_tokens',\n",
    " 'model_name',\n",
    " 'parse_response',\n",
    " 'repetition_penalty',\n",
    " 'run_all',\n",
    " 'sample()',\n",
    " 'stochastic_balance_dataset',\n",
    " 'submit_job',\n",
    " 'test_cache',\n",
    " 'test_output_str',\n",
    " 'test_parallel_complex',\n",
    " 'test_parallel_simple',\n",
    " 'value_to_dict',\n",
    " 'write_story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = []\n",
    "\n",
    "for func in filtered_entities:\n",
    "    _context = []\n",
    "    for qa in data:\n",
    "        if func in qa:\n",
    "            _context.append(qa)\n",
    "    context.append([func, ' '.join(_context)])\n",
    "\n",
    "import pickle as pkl\n",
    "pkl.dump(context, open(\"context.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "file_path = 'context.csv'\n",
    "\n",
    "# Writing to a CSV file\n",
    "with open(file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Entity\", \"Context\"])\n",
    "    writer.writerows(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pkl.load(open(\"summary.pkl\", \"rb\"))\n",
    "\n",
    "file_path = 'summary.csv'\n",
    "\n",
    "# Writing to a CSV file\n",
    "with open(file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Entity\", \"Summary\"])\n",
    "    writer.writerows(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
