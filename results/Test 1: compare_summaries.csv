Entities,is_common,Actual Summary,Generated Summary,is_match
add_data,1,"The `add_data()` function in Lamini allows you to add training data to the LLM Engine, enabling the model to learn and improve its performance.","The add_data function/parameter allows for the addition of new data to the LLM Engine, which can be used to improve its performance and accuracy. 30 words.",1
add_improve_statements,0,"In Lamini, the `add_improve_statements()` function allows adding multiple improve statements to the LLM engine.",The add_improve_statements function/parameter allows for the addition of new statements to improve the model's performance. 30 words.,1
add_metric,1,"The `add_metric` method in Lamini allows you to add a metric to compare the generated output to a target output, and train the LLM Engine using the `fit` method.",The add_metric function/parameter allows users to track and evaluate the performance of LLMs on specific tasks or metrics. 30 words.,0
add_model,1,"The ""add_model"" method in Lamini's python library allows for creating multiple models with different parameters and output types, which can be used to fine-tune the model on a specific dataset for dialogue generation tasks.",The add_model function/parameter allows for the addition of multiple models to the LLM Engine in Lamini. 30 words.,1
bad_examples,0,The `bad_examples` list provides the model with incorrect or undesired output examples to learn from and improve its performance.,The bad_examples function/parameter takes in a list of examples and returns a list of examples that do not follow the guidelines set by the Lamini team. 30 words,0
cancel_job,1,"The `cancel_job()` function in Lamini stops a running job by sending a request to the server, releasing any resources used, and returning a response indicating the cancellation's success or failure.",The cancel_job function/parameter allows for the cancellation of a job that is currently running. This can be useful in situations where the job is no longer needed or has encountered an error. 30 words.,1
check_job_status,1,"The `check_job_status()` function in Lamini provides real-time updates on the status of a job, including running, completed, or failed, and may provide additional details such as start and end times and error messages.","The check_job_status function/parameter is used to check the status of a job submitted to the Lamini platform. It returns information about the job's progress, such as its current status and any errors that may have occurred during processing. 30 words.",1
compare_equal_metric,0,"The ""compare_equal_metric.py"" file provides code samples for defining and adding custom metrics to a program for execution by the Llama large language model engine.",The compare_equal_metric function/parameter compares two metrics based on their similarity or dissimilarity. It takes two metrics as input and returns a score indicating how similar or dissimilar they are. 30 words.,0
configure_llama,1,The `~/.powerml/configure_llama.yaml` file stores the Lamini API key for automatic loading by the Lamini Python package.,"The configure_llama function/parameter allows for the customization of the Llama model's hyperparameters, such as learning rate and batch size, during the training process. 30 words.",0
edit_config,1,"The ""edit_config"" function in Lamini allows users to update configuration settings, including model updates or retraining, without disrupting their software application.","The edit_config function/parameter allows for the customization of the LLM engine's configuration settings, enabling developers to tailor the LLM's behavior and performance to their specific needs.",1
error_handling,1,"The ""error_handling"" function in Lamini's LLM Engine provides guidelines and resources for handling errors and exceptions in the code, including logging and reporting mechanisms.",The error_handling function/parameter is used to handle errors and exceptions that may occur during the execution of the code. It allows for customized error handling and provides a way to handle unexpected errors in a structured manner. 30 words.,1
filter_fn,1,The `filter_fn` parameter allows for custom filtering of generated output based on a provided function.,"The filter_fn function/parameter allows for the filtering of generated text based on specific criteria, such as relevance or quality. 30 words.",1
full_balance_dataset,0,"The ""full_balance_dataset"" method in Lamini's DatasetBalancer class balances a dataset without random sampling, using embeddings to compare data points and remove duplicates based on a threshold.",The full_balance_dataset function/parameter in Lamini's LLM Engine is used to balance the training data by randomly removing 50% of the less represented classes. This ensures that the model is not biased towards any particular class during training. 30 words.,0
gen_queue_batch,0,"The `gen_queue_batch()` function in Lamini allows you to queue a batch of values for processing, enabling you to track the status of the job through the `check_job_status()` function.",The gen_queue_batch function/parameter is used to generate a batch of input and output pairs for training LLMs. It takes in a queue of input and output pairs and outputs a batch of them. 30 words.,0
gen_submit_training_job,0,The `gen_submit_training_job()` function in Lamini submits a training job to the platform for processing.,The `gen_submit_training_job` function generates a job for training a model using the Llama large language model engine. It takes in a training_data parameter and outputs a job ID. 30 words.,0
get_job_results,1,The `get_job_results` method in Lamini retrieves the results of a previously submitted job.,The get_job_results function/parameter allows for the retrieval of the results of a job that has been submitted to the Lamini engine. 30 words.,1
get_response,1,"""get_response"" retrieves a response from the Llama API and handles errors by retrying up to 5 times before raising a runtime error.",The get_response function/parameter takes in a user’s input and returns a response based on that input.  It is used internally by the LLM Engine to generate responses to user input.  It is not exposed to the user and is only used internally by the LLM Engine to generate responses to user input.  It is not exposed to the user and is only used internally by the LLM Engine to generate responses to user input.  It is not,1
good_examples,0,"The `good_examples` argument in Lamini's `improve()` function provides the model with a list of high-quality examples of the desired output, which the model can learn from to improve its performance.",The good_examples function/parameter takes in a list of str and returns a list of str.  It filters out any examples that do not follow the guidelines set out in the LLM Engine.  It also filters out any examples that do not follow the guidelines set out in the LLM Engine.  It filters out any examples that do not follow the guidelines set out in the LLM Engine.  It filters out any examples that,0
improve(),0,"The `improve()` function in Lamini fine-tunes a model's output by providing good and bad examples of the desired output, allowing the model to learn from its mistakes and improve its performance.",The improve() function/parameter allows for the fine-tuning of the model's output based on user feedback. 30 words.,1
is_peft_model,1,"The ""is_peft_model"" parameter in Lamini allows for better interpretability of customized LLMs by enabling the Partially Extractive Fine-Tuning (PEFT) method, which improves the model's ability to explain its predictions.",The is_peft_model function/parameter is used to determine if the input data is suitable for training a PEFT model. It takes in a string representing the input data and returns a boolean value indicating whether it is a good fit for PEFT training. 30 words.,0
length_penalty,1,"In Lamini, ""length_penalty"" refers to a parameter that penalizes generated text that exceeds a specified length, encouraging the model to produce concise output.","The length_penalty function/parameter is used to adjust the score of a generated text based on its length. The higher the length, the lower the score. 30 words.",1
llm(),0,"The `llm()` function in Lamini allows for the creation of customized language models with interpretability and explainability features, such as the PEFT method and parse_response function.",The llm() function in Lamini’s python library is used to create a Large Language Model (LLM) based on a given dataset. It takes in the dataset and outputs a trained LLM model that can be used for natural language processing tasks. 30 words.,1
make_discriminator,1,"""Make_discriminator"" function in Lamini provides options for different model types and hyperparameter tuning using GridSearchCV to evaluate the performance of a customized discriminator model.",The make_discriminator function/parameter in Lamini allows for the creation of discriminators that can differentiate between input and output pairs based on specific criteria. 30 words.,1
make_questions,1,"""Make questions"" generates customized questions based on both text and other types of data for multimodal learning in Lamini.","The make_questions function/parameter takes in a type and returns a list of questions that can be asked based on that type.  It is used internally by the LLM Engine to generate questions based on the input data.  For example, if the input data is a description of a cat, the make_questions function would return a list of questions such as ""What color is the cat?"", ""How many legs does the cat have?"", etc.",1
max_retries,1,The `max_retries` parameter allows for limiting the number of retries in case the generated output does not satisfy a specified condition.,The max_retries parameter sets the maximum number of retries before raising an error. 30,1
max_tokens,1,"The ""max_tokens"" parameter in Lamini controls the maximum length of generated text outputs, allowing for precise control over text length.",The max_tokens function/parameter limits the maximum number of tokens that can be generated by the LLM Engine in a given task. It helps to prevent overfitting and improve the quality of the generated output.,1
model_name,1,"The ""model_name"" parameter in Lamini allows users to specify the name of a pre-trained language model to use for text generation, customization, or inference.",The model_name function/parameter is used to specify the name of the trained LLM model in Lamini. 30 words.,1
parse_response,1,The `parse_response` function strips leading and trailing whitespace from the response string and extracts the most relevant information from the model's output.,The parse_response function/parameter takes in a response from the LLM engine and parses it to extract the relevant information. 30 words.,1
repetition_penalty,1,"The ""repetition_penalty"" parameter in Lamini allows for controlling the repetition of generated text by imposing a penalty on repeated words or phrases.",The repetition_penalty function/parameter in Lamini's python library is used to adjust the score of generated text based on the frequency of certain words or phrases. It helps to encourage diversity and originality in the generated text.,1
run_all,0,"""Runs all models in parallel using the @llm.parallel decorator and the llama.run_all method.""",The run_all function/parameter allows for the execution of multiple LLM training jobs simultaneously. 30,1
sample(),0,"The `sample()` function in Lamini generates text outputs based on a given prompt or context, using a pre-trained language model to predict the most likely next word or sequence of words.",The sample() function/parameter takes in a type and returns a random example of that type.  It can be used to generate a diverse set of examples that can be used for training or testing purposes.  It can also be used to generate a diverse set of examples that can be used for training or testing purposes.  It can be used to generate a diverse set of examples that can be used for training or testing purposes.  It can be used to generate a,0
stochastic_balance_dataset,0,"The `stochastic_balance_dataset` method in `balancer.py` randomly samples from a balanced index to add new data points to a small dataset with limited annotations, improving the performance of the model.","The stochastic_balance_dataset function/parameter is used to balance the training data in a stochastic manner, ensuring that the model is exposed to a diverse range of examples during training. 30 words.",0
submit_job,1,"The `submit_job()` function in Lamini submits a machine learning training job to the cluster for processing, handling background processing and job scheduling.",The submit_job function/parameter allows for the submission of a job for execution by the Llama large language model engine. 30 words.,1
test_cache,0,"In the provided code, the ""test_cache"" function compares the execution time of the code with and without caching to test the effectiveness of the caching mechanism.",The test_cache function/parameter is used to test the caching functionality of the Lamini library. It takes in a str and returns a tuple containing the original str and the cached str. 30 words.,0
test_output_str,0,"In the ""test_output_str"" function, the level of specificity in the generated output can be customized by adjusting input parameters and output types in the LLM Engine function.",The test_output_str function/parameter takes a string as input and outputs a generated story based on the given input. 30 words.,0
test_parallel_complex,0,"In the ""test_parallel_complex"" method, the `circular_operation` function is parallelized using the `llm.parallel` decorator, and all models are run in parallel using the `llama.run_all` method.",The test_parallel_complex function/parameter is used to test the parallelization of complex operations in Lamini. 30 words.,0
test_parallel_simple,0,"In the ""test_parallel_simple"" method, multiple models are run in parallel using the `llama.run_all` method to test the code's ability to handle concurrency and parallelism.",The test_parallel_simple function/parameter is used to test the parallelism of the LLM engine with a simple input. 30 words.,0
value_to_dict,1,The `value_to_dict()` function in Lamini converts examples to a dictionary format for training data.,"The value_to_dict function/parameter takes a value object and converts it into a dictionary format.  It is used internally by the LLM Engine and is not intended for external use.  If you think this information is incorrect or out of date, please open a support ticket at https://lamini.ai/support  for more information.  Otherwise, continue with the documentation.  You are on section 3 of 10 in the documentation.",1
write_story,1,"The ""write_story"" function in Lamini's python library generates a story based on user input, allowing for fine-grained control over creativity and randomness.","The write_story function/parameter allows for the creation of a narrative or story based on a given prompt or input. It enables the generation of a coherent and engaging story that can be used for various purposes, such as entertainment or marketing. 30 words.",1