{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing context summaries [RAG] with generated summaries [Finetuned ZS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% match in summaries: 61.54%\n"
     ]
    }
   ],
   "source": [
    "compare_summaries = pd.read_csv('compare_summaries.csv')\n",
    "\n",
    "is_match = list(compare_summaries['is_match']) \n",
    "is_common = list(compare_summaries['is_common'])\n",
    "print(f\"% match in summaries: {np.round(sum(is_match)/len(is_match) * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  4]\n",
      " [ 3 20]]\n",
      "Total number of functions understood: 61.54%\n",
      "Total number of common functions understood: 86.96%\n",
      "Total number of corpus-specific functions understood: 25.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(is_common, is_match)\n",
    "print(cm)\n",
    "\n",
    "print(f\"Total number of functions understood: {np.round(sum(is_match)/len(is_match) * 100, 2)}%\")\n",
    "print(f\"Total number of common functions understood: {np.round(cm[1][1]/(cm[1][0] + cm[1][1]) * 100, 2)}%\")\n",
    "print(f\"Total number of corpus-specific functions understood: {np.round(cm[0][1]/(cm[0][0] + cm[0][1]) * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_understood = list(compare_summaries[(compare_summaries['is_match'] == 1) & (compare_summaries['is_common'] == 0)][\"Entities\"])\n",
    "common_understood = list(compare_summaries[(compare_summaries['is_match'] == 1) & (compare_summaries['is_common'] == 1)][\"Entities\"])\n",
    "\n",
    "corpus = list(compare_summaries[compare_summaries['is_common'] == 0][\"Entities\"])\n",
    "common = list(compare_summaries[compare_summaries['is_common'] == 1][\"Entities\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"lamini/lamini_docs\"\n",
    "datasets = load_dataset(dataset_name)\n",
    "\n",
    "data = []\n",
    "\n",
    "for qa_pair in datasets['train']:\n",
    "    _sentence = (qa_pair['question'] + ' ' + qa_pair['answer']).replace(\"\\\\n\", \" \")\n",
    "    data.append(_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "entities = list(compare_summaries[\"Entities\"])\n",
    "\n",
    "count = {}\n",
    "\n",
    "for e in entities:\n",
    "    _count = 0\n",
    "    for d in data:\n",
    "        if e in d:\n",
    "            _count += 1\n",
    "    count[e] = _count\n",
    "\n",
    "understood_count = []\n",
    "not_understood_count = []\n",
    "\n",
    "for e in corpus_understood:\n",
    "    understood_count.append(count[e])\n",
    "\n",
    "for e in list(set(corpus) - set(corpus_understood)):\n",
    "    not_understood_count.append(count[e])\n",
    "\n",
    "plt.boxplot([understood_count, not_understood_count], labels=['Understood', 'Not Understood'], showfliers=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Inverted Question Answers with Actual Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"summary.csv\")\n",
    "expected_answers = list(df[\"Entity\"])\n",
    "summaries = list(df[\"Summary\"])\n",
    "\n",
    "generated_answers = pkl.load(open(\"inverted_questions_answers.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common functions retrieved: 45.0%\n",
      "Corpus functions retrieved: 0.0%\n",
      "Total functions retrieved: 37.5%\n"
     ]
    }
   ],
   "source": [
    "match = 0\n",
    "matched_entities = []\n",
    "matched_summary = []\n",
    "\n",
    "for (e, g, s) in zip(expected_answers, generated_answers, summaries):\n",
    "    if e.lower() in g.lower():\n",
    "        match += 1\n",
    "        matched_entities.append(e)\n",
    "        matched_summary.append(s)\n",
    "\n",
    "print(f\"Common functions retrieved: {np.round(len(set(matched_entities).intersection(common_understood))/len(common_understood) * 100, 2)}%\")\n",
    "print(f\"Corpus functions retrieved: {np.round(len(set(matched_entities).intersection(corpus_understood))/len(corpus_understood) * 100, 2)}%\")\n",
    "print(f\"Total functions retrieved: {np.round(len(matched_entities)/(len(corpus_understood) + len(common_understood)) * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Confidence Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What function in Lamini terminates a running job by sending a request to the server, releasing any resources used, and returning a response indicating the cancellation's success or failure?\",\n",
       " 'What function in Lamini offers real-time updates on the status of a job, including its progress, completion, or failure, and may provide additional details such as timestamps and error messages?',\n",
       " 'What parameter allows for custom filtering of generated output based on a provided function?',\n",
       " 'What function in Lamini retrieves a response from the Llama API and handles errors by retrying up to 5 times before raising a runtime error?',\n",
       " 'What parameter in Lamini limits the number of retries in case the generated output does not satisfy a specified condition?',\n",
       " 'What parameter in Lamini controls the maximum length of generated text outputs, allowing for precise control over text length?',\n",
       " 'What parameter in Lamini allows users to specify the name of a pre-trained language model to use for text generation, customization, or inference?',\n",
       " 'What function in Lamini submits a machine learning training job to the cluster for processing, handling background processing and job scheduling?',\n",
       " \"What function in Lamini's python library generates a story based on user input, allowing for fine-grained control over creativity and randomness?\"]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_inverted_questions = []\n",
    "\n",
    "for e in matched_entities:\n",
    "    inverted_question = df.loc[df[\"Entity\"] == e][\"Inverted Questions\"].iloc[0]\n",
    "    match_inverted_questions.append(inverted_question)\n",
    "\n",
    "match_inverted_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('matched_entities.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Entities\", \"Summary\", \"Inverted Questions\"])\n",
    "    for x, y, z in zip(matched_entities, matched_summary, match_inverted_questions):\n",
    "        writer.writerow([x, y, z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of correct answers from the FCT : 11.11%\n",
      "['filter_fn']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"matched_entities.csv\")\n",
    "expected_answers = list(df[\"Entities\"])\n",
    "\n",
    "generated_answers = pkl.load(open(\"fct_answers.pkl\", \"rb\"))\n",
    "\n",
    "correct_answers = 0\n",
    "correct_entity = []\n",
    "\n",
    "for (e, g) in zip(expected_answers, generated_answers):\n",
    "    if e.lower() in g.lower():\n",
    "        correct_answers += 1\n",
    "        correct_entity.append(e)\n",
    "\n",
    "print(f\"% of correct answers from the FCT : {np.round(correct_answers/len(expected_answers) * 100, 2)}%\")\n",
    "print(correct_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning Abilities\n",
    "\n",
    "\"\"\"\n",
    "Enter the prompt: Are both .query() in H2O.ai and filter_fn in Lamini used for filtering data?\n",
    " \n",
    "### Answer: Yes, both H2O.ai's.query() and Lamini's filter_fn are used for filtering data. .query() in H2O.ai is used to create a query that filters the data, while filter_fn in Lamini is a function that takes in a data generator and returns a filtered data generator.  Both are used to select specific data points that meet certain criteria, such as filtering out outliers or selecting only\n",
    "********************************************************************************\n",
    "\n",
    "Enter the prompt: Is there any similarity between the .query() in H2O.ai and filter_fn in Lamini?\n",
    " \n",
    "### Answer: There is no similarity between the.query() in H2O.ai and filter_fn in Lamini as they serve different purposes. The.query() method in H2O.ai is used to specify the query parameters for a query operation, while the filter_fn parameter in Lamini is used to define a filter function that is applied to the input data before it is passed to the LLM engine. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing generated summary with actual summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_summary = pkl.load(open(\"generated_summary.pkl\", \"rb\"))   \n",
    "\n",
    "clean_summary = []\n",
    "for x in generated_summary:\n",
    "    clean_summary.append(x.split('###')[1][9:].strip())\n",
    "\n",
    "acutal_summary = list(df[\"Summary\"])\n",
    "actual_summary = [x .strip() for x in acutal_summary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The add_data function/parameter allows for the addition of new data to the LLM Engine, which can be used to improve its performance and accuracy. 30 words.\n",
      "The `add_data()` function in Lamini allows you to add training data to the LLM Engine, enabling the model to learn and improve its performance.\n",
      "\n",
      "\n",
      "The add_improve_statements function/parameter allows for the addition of new statements to improve the model's performance. 30 words.\n",
      "In Lamini, the `add_improve_statements()` function allows adding multiple improve statements to the LLM engine.\n",
      "\n",
      "\n",
      "The add_metric function/parameter allows users to track and evaluate the performance of LLMs on specific tasks or metrics. 30 words.\n",
      "The `add_metric` method in Lamini allows you to add a metric to compare the generated output to a target output, and train the LLM Engine using the `fit` method.\n",
      "\n",
      "\n",
      "The add_model function/parameter allows for the addition of multiple models to the LLM Engine in Lamini. 30 words.\n",
      "The \"add_model\" method in Lamini's python library allows for creating multiple models with different parameters and output types, which can be used to fine-tune the model on a specific dataset for dialogue generation tasks.\n",
      "\n",
      "\n",
      "The bad_examples function/parameter takes in a list of examples and returns a list of examples that do not follow the guidelines set by the Lamini team. 30 words\n",
      "The `bad_examples` list provides the model with incorrect or undesired output examples to learn from and improve its performance.\n",
      "\n",
      "\n",
      "The cancel_job function/parameter allows for the cancellation of a job that is currently running. This can be useful in situations where the job is no longer needed or has encountered an error. 30 words.\n",
      "The `cancel_job()` function in Lamini stops a running job by sending a request to the server, releasing any resources used, and returning a response indicating the cancellation's success or failure.\n",
      "\n",
      "\n",
      "The check_job_status function/parameter is used to check the status of a job submitted to the Lamini platform. It returns information about the job's progress, such as its current status and any errors that may have occurred during processing. 30 words.\n",
      "The `check_job_status()` function in Lamini provides real-time updates on the status of a job, including running, completed, or failed, and may provide additional details such as start and end times and error messages.\n",
      "\n",
      "\n",
      "The compare_equal_metric function/parameter compares two metrics based on their similarity or dissimilarity. It takes two metrics as input and returns a score indicating how similar or dissimilar they are. 30 words.\n",
      "The \"compare_equal_metric.py\" file provides code samples for defining and adding custom metrics to a program for execution by the Llama large language model engine.\n",
      "\n",
      "\n",
      "The configure_llama function/parameter allows for the customization of the Llama model's hyperparameters, such as learning rate and batch size, during the training process. 30 words.\n",
      "The `~/.powerml/configure_llama.yaml` file stores the Lamini API key for automatic loading by the Lamini Python package.\n",
      "\n",
      "\n",
      "The edit_config function/parameter allows for the customization of the LLM engine's configuration settings, enabling developers to tailor the LLM's behavior and performance to their specific needs.\n",
      "The \"edit_config\" function in Lamini allows users to update configuration settings, including model updates or retraining, without disrupting their software application.\n",
      "\n",
      "\n",
      "The error_handling function/parameter is used to handle errors and exceptions that may occur during the execution of the code. It allows for customized error handling and provides a way to handle unexpected errors in a structured manner. 30 words.\n",
      "The \"error_handling\" function in Lamini's LLM Engine provides guidelines and resources for handling errors and exceptions in the code, including logging and reporting mechanisms.\n",
      "\n",
      "\n",
      "The filter_fn function/parameter allows for the filtering of generated text based on specific criteria, such as relevance or quality. 30 words.\n",
      "The `filter_fn` parameter allows for custom filtering of generated output based on a provided function.\n",
      "\n",
      "\n",
      "The full_balance_dataset function/parameter in Lamini's LLM Engine is used to balance the training data by randomly removing 50% of the less represented classes. This ensures that the model is not biased towards any particular class during training. 30 words.\n",
      "The \"full_balance_dataset\" method in Lamini's DatasetBalancer class balances a dataset without random sampling, using embeddings to compare data points and remove duplicates based on a threshold.\n",
      "\n",
      "\n",
      "The gen_queue_batch function/parameter is used to generate a batch of input and output pairs for training LLMs. It takes in a queue of input and output pairs and outputs a batch of them. 30 words.\n",
      "The `gen_queue_batch()` function in Lamini allows you to queue a batch of values for processing, enabling you to track the status of the job through the `check_job_status()` function.\n",
      "\n",
      "\n",
      "The `gen_submit_training_job` function generates a job for training a model using the Llama large language model engine. It takes in a training_data parameter and outputs a job ID. 30 words.\n",
      "The `gen_submit_training_job()` function in Lamini submits a training job to the platform for processing.\n",
      "\n",
      "\n",
      "The get_job_results function/parameter allows for the retrieval of the results of a job that has been submitted to the Lamini engine. 30 words.\n",
      "The `get_job_results` method in Lamini retrieves the results of a previously submitted job.\n",
      "\n",
      "\n",
      "The get_response function/parameter takes in a user’s input and returns a response based on that input.  It is used internally by the LLM Engine to generate responses to user input.  It is not exposed to the user and is only used internally by the LLM Engine to generate responses to user input.  It is not exposed to the user and is only used internally by the LLM Engine to generate responses to user input.  It is not\n",
      "\"get_response\" retrieves a response from the Llama API and handles errors by retrying up to 5 times before raising a runtime error.\n",
      "\n",
      "\n",
      "The good_examples function/parameter takes in a list of str and returns a list of str.  It filters out any examples that do not follow the guidelines set out in the LLM Engine.  It also filters out any examples that do not follow the guidelines set out in the LLM Engine.  It filters out any examples that do not follow the guidelines set out in the LLM Engine.  It filters out any examples that\n",
      "The `good_examples` argument in Lamini's `improve()` function provides the model with a list of high-quality examples of the desired output, which the model can learn from to improve its performance.\n",
      "\n",
      "\n",
      "The improve() function/parameter allows for the fine-tuning of the model's output based on user feedback. 30 words.\n",
      "The `improve()` function in Lamini fine-tunes a model's output by providing good and bad examples of the desired output, allowing the model to learn from its mistakes and improve its performance.\n",
      "\n",
      "\n",
      "The is_peft_model function/parameter is used to determine if the input data is suitable for training a PEFT model. It takes in a string representing the input data and returns a boolean value indicating whether it is a good fit for PEFT training. 30 words.\n",
      "The \"is_peft_model\" parameter in Lamini allows for better interpretability of customized LLMs by enabling the Partially Extractive Fine-Tuning (PEFT) method, which improves the model's ability to explain its predictions.\n",
      "\n",
      "\n",
      "The length_penalty function/parameter is used to adjust the score of a generated text based on its length. The higher the length, the lower the score. 30 words.\n",
      "In Lamini, \"length_penalty\" refers to a parameter that penalizes generated text that exceeds a specified length, encouraging the model to produce concise output.\n",
      "\n",
      "\n",
      "The llm() function in Lamini’s python library is used to create a Large Language Model (LLM) based on a given dataset. It takes in the dataset and outputs a trained LLM model that can be used for natural language processing tasks. 30 words.\n",
      "The `llm()` function in Lamini allows for the creation of customized language models with interpretability and explainability features, such as the PEFT method and parse_response function.\n",
      "\n",
      "\n",
      "The make_discriminator function/parameter in Lamini allows for the creation of discriminators that can differentiate between input and output pairs based on specific criteria. 30 words.\n",
      "\"Make_discriminator\" function in Lamini provides options for different model types and hyperparameter tuning using GridSearchCV to evaluate the performance of a customized discriminator model.\n",
      "\n",
      "\n",
      "The make_questions function/parameter takes in a type and returns a list of questions that can be asked based on that type.  It is used internally by the LLM Engine to generate questions based on the input data.  For example, if the input data is a description of a cat, the make_questions function would return a list of questions such as \"What color is the cat?\", \"How many legs does the cat have?\", etc.\n",
      "\"Make questions\" generates customized questions based on both text and other types of data for multimodal learning in Lamini.\n",
      "\n",
      "\n",
      "The max_retries parameter sets the maximum number of retries before raising an error. 30\n",
      "The `max_retries` parameter allows for limiting the number of retries in case the generated output does not satisfy a specified condition.\n",
      "\n",
      "\n",
      "The max_tokens function/parameter limits the maximum number of tokens that can be generated by the LLM Engine in a given task. It helps to prevent overfitting and improve the quality of the generated output.\n",
      "The \"max_tokens\" parameter in Lamini controls the maximum length of generated text outputs, allowing for precise control over text length.\n",
      "\n",
      "\n",
      "The model_name function/parameter is used to specify the name of the trained LLM model in Lamini. 30 words.\n",
      "The \"model_name\" parameter in Lamini allows users to specify the name of a pre-trained language model to use for text generation, customization, or inference.\n",
      "\n",
      "\n",
      "The parse_response function/parameter takes in a response from the LLM engine and parses it to extract the relevant information. 30 words.\n",
      "The `parse_response` function strips leading and trailing whitespace from the response string and extracts the most relevant information from the model's output.\n",
      "\n",
      "\n",
      "The repetition_penalty function/parameter in Lamini's python library is used to adjust the score of generated text based on the frequency of certain words or phrases. It helps to encourage diversity and originality in the generated text.\n",
      "The \"repetition_penalty\" parameter in Lamini allows for controlling the repetition of generated text by imposing a penalty on repeated words or phrases.\n",
      "\n",
      "\n",
      "The run_all function/parameter allows for the execution of multiple LLM training jobs simultaneously. 30\n",
      "\"Runs all models in parallel using the @llm.parallel decorator and the llama.run_all method.\"\n",
      "\n",
      "\n",
      "The sample() function/parameter takes in a type and returns a random example of that type.  It can be used to generate a diverse set of examples that can be used for training or testing purposes.  It can also be used to generate a diverse set of examples that can be used for training or testing purposes.  It can be used to generate a diverse set of examples that can be used for training or testing purposes.  It can be used to generate a\n",
      "The `sample()` function in Lamini generates text outputs based on a given prompt or context, using a pre-trained language model to predict the most likely next word or sequence of words.\n",
      "\n",
      "\n",
      "The stochastic_balance_dataset function/parameter is used to balance the training data in a stochastic manner, ensuring that the model is exposed to a diverse range of examples during training. 30 words.\n",
      "The `stochastic_balance_dataset` method in `balancer.py` randomly samples from a balanced index to add new data points to a small dataset with limited annotations, improving the performance of the model.\n",
      "\n",
      "\n",
      "The submit_job function/parameter allows for the submission of a job for execution by the Llama large language model engine. 30 words.\n",
      "The `submit_job()` function in Lamini submits a machine learning training job to the cluster for processing, handling background processing and job scheduling.\n",
      "\n",
      "\n",
      "The test_cache function/parameter is used to test the caching functionality of the Lamini library. It takes in a str and returns a tuple containing the original str and the cached str. 30 words.\n",
      "In the provided code, the \"test_cache\" function compares the execution time of the code with and without caching to test the effectiveness of the caching mechanism.\n",
      "\n",
      "\n",
      "The test_output_str function/parameter takes a string as input and outputs a generated story based on the given input. 30 words.\n",
      "In the \"test_output_str\" function, the level of specificity in the generated output can be customized by adjusting input parameters and output types in the LLM Engine function.\n",
      "\n",
      "\n",
      "The test_parallel_complex function/parameter is used to test the parallelization of complex operations in Lamini. 30 words.\n",
      "In the \"test_parallel_complex\" method, the `circular_operation` function is parallelized using the `llm.parallel` decorator, and all models are run in parallel using the `llama.run_all` method.\n",
      "\n",
      "\n",
      "The test_parallel_simple function/parameter is used to test the parallelism of the LLM engine with a simple input. 30 words.\n",
      "In the \"test_parallel_simple\" method, multiple models are run in parallel using the `llama.run_all` method to test the code's ability to handle concurrency and parallelism.\n",
      "\n",
      "\n",
      "The value_to_dict function/parameter takes a value object and converts it into a dictionary format.  It is used internally by the LLM Engine and is not intended for external use.  If you think this information is incorrect or out of date, please open a support ticket at https://lamini.ai/support  for more information.  Otherwise, continue with the documentation.  You are on section 3 of 10 in the documentation.\n",
      "The `value_to_dict()` function in Lamini converts examples to a dictionary format for training data.\n",
      "\n",
      "\n",
      "The write_story function/parameter allows for the creation of a narrative or story based on a given prompt or input. It enables the generation of a coherent and engaging story that can be used for various purposes, such as entertainment or marketing. 30 words.\n",
      "The \"write_story\" function in Lamini's python library generates a story based on user input, allowing for fine-grained control over creativity and randomness.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (c, a) in zip(clean_summary, actual_summary):\n",
    "    print(c)\n",
    "    print(a)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
